{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4a1d51",
   "metadata": {},
   "source": [
    "# Indian House Price Prediction Model\n",
    "\n",
    "**A simplified, production-ready ML pipeline for house price prediction**\n",
    "\n",
    "- Clean data preprocessing\n",
    "- Feature engineering\n",
    "- Random Forest model training\n",
    "- Model evaluation\n",
    "- Model saving with joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669be41",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06535950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159e372",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "For Google Colab: Change the path to `\"india_housing_prices.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ebe8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Data_Science_Programme\\\\Machine_learning_models\\\\Indian_House_price_prediction\\\\india_housing_prices.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3299072896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Data_Science_Programme\\Machine_learning_models\\Indian_House_price_prediction\\india_housing_prices.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Data_Science_Programme\\\\Machine_learning_models\\\\Indian_House_price_prediction\\\\india_housing_prices.csv'"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(r\"\")\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07adaa86",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d4fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533de38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns\n",
    "print(\"All columns in dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Drop unnecessary columns\n",
    "# ID - not useful, Price_per_SqFt - can be derived, Age_of_Property - duplicate of Year_Built\n",
    "columns_to_drop = ['ID', 'Price_per_SqFt', 'Age_of_Property']\n",
    "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values!\")\n",
    "print(f\"\\nDataset shape after dropping columns: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b56808",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Property_Age feature (if not already present)\n",
    "CURRENT_YEAR = 2025\n",
    "if 'Year_Built' in df.columns:\n",
    "    df['Property_Age'] = CURRENT_YEAR - df['Year_Built']\n",
    "    df = df.drop(columns=['Year_Built'])\n",
    "\n",
    "# Create price per sqft feature (useful for analysis)\n",
    "df['Price_Per_SqFt'] = df['Price_in_Lakhs'] / df['Size_in_SqFt']\n",
    "\n",
    "# Create floor ratio feature\n",
    "if 'Floor_No' in df.columns and 'Total_Floors' in df.columns:\n",
    "    df['Floor_Ratio'] = df['Floor_No'] / df['Total_Floors'].replace(0, 1)\n",
    "\n",
    "# Binary encode Yes/No columns\n",
    "binary_cols = ['Parking_Space', 'Security']\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map({'Yes': 1, 'No': 0}).fillna(0)\n",
    "\n",
    "# Encode categorical columns with ordering\n",
    "if 'Public_Transport_Accessibility' in df.columns:\n",
    "    df['Public_Transport_Accessibility'] = df['Public_Transport_Accessibility'].map({\n",
    "        'High': 3, 'Medium': 2, 'Low': 1\n",
    "    }).fillna(1)\n",
    "\n",
    "if 'Furnished_Status' in df.columns:\n",
    "    df['Furnished_Status'] = df['Furnished_Status'].map({\n",
    "        'Furnished': 3, 'Semi-furnished': 2, 'Unfurnished': 1\n",
    "    }).fillna(1)\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "print(f\"New features created: Property_Age, Price_Per_SqFt, Floor_Ratio\")\n",
    "print(f\"Property Age range: {df['Property_Age'].min()} to {df['Property_Age'].max()} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504b976",
   "metadata": {},
   "source": [
    "## 5. Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers using IQR method\n",
    "Q1 = df['Price_in_Lakhs'].quantile(0.25)\n",
    "Q3 = df['Price_in_Lakhs'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "df = df[(df['Price_in_Lakhs'] >= lower) & (df['Price_in_Lakhs'] <= upper)]\n",
    "\n",
    "# Remove anomalies (large houses with very low prices)\n",
    "df = df[~((df['Size_in_SqFt'] > 2500) & (df['Price_in_Lakhs'] < 20))]\n",
    "\n",
    "print(f\"After outlier removal: {df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa16d6",
   "metadata": {},
   "source": [
    "## 6. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "# First, drop Price_Per_SqFt from features (we created it but don't want to use it for prediction)\n",
    "X = df.drop(columns=['Price_in_Lakhs', 'Price_Per_SqFt'])\n",
    "y = df['Price_in_Lakhs']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures being used:\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899d104",
   "metadata": {},
   "source": [
    "## 7. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61287cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "\n",
    "# Handle complex categorical columns\n",
    "# For Amenities column - count number of amenities\n",
    "if 'Amenities' in X.columns:\n",
    "    X['Amenities_Count'] = X['Amenities'].str.split(',').str.len()\n",
    "    categorical_cols.remove('Amenities')\n",
    "\n",
    "# Reduce high cardinality features (keep top 10 categories)\n",
    "def reduce_cardinality(df, col, top_n=10):\n",
    "    top_categories = df[col].value_counts().head(top_n).index\n",
    "    df[col] = df[col].apply(lambda x: x if x in top_categories else 'Other')\n",
    "    return df\n",
    "\n",
    "# Apply to high cardinality columns\n",
    "X_processed = X.copy()\n",
    "high_cardinality_cols = ['Locality', 'City', 'State']\n",
    "\n",
    "for col in high_cardinality_cols:\n",
    "    if col in X_processed.columns and col in categorical_cols:\n",
    "        original_unique = X_processed[col].nunique()\n",
    "        X_processed = reduce_cardinality(X_processed, col, top_n=10)\n",
    "        print(f\"{col}: Reduced from {original_unique} to {X_processed[col].nunique()} categories\")\n",
    "\n",
    "# Drop Locality (too granular, City and State are enough)\n",
    "if 'Locality' in X_processed.columns:\n",
    "    X_processed = X_processed.drop(columns=['Locality'])\n",
    "    if 'Locality' in categorical_cols:\n",
    "        categorical_cols.remove('Locality')\n",
    "    print(\"Dropped 'Locality' column (too granular)\")\n",
    "\n",
    "# One-hot encoding for remaining categorical columns\n",
    "X_encoded = pd.get_dummies(X_processed, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\nFinal encoded features: {X_encoded.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d4a7e",
   "metadata": {},
   "source": [
    "## 8. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b8de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4785b",
   "metadata": {},
   "source": [
    "## 9. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ed2dd",
   "metadata": {},
   "source": [
    "## 10. Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eeea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6426e8",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# MAPE\n",
    "mape = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train RÂ² Score: {train_r2:.4f}\")\n",
    "print(f\"Test RÂ² Score:  {test_r2:.4f}\")\n",
    "print(f\"\\nTrain RMSE: â‚¹{train_rmse:.2f} Lakhs\")\n",
    "print(f\"Test RMSE:  â‚¹{test_rmse:.2f} Lakhs\")\n",
    "print(f\"\\nTrain MAE: â‚¹{train_mae:.2f} Lakhs\")\n",
    "print(f\"Test MAE:  â‚¹{test_mae:.2f} Lakhs\")\n",
    "print(f\"\\nMAPE: {mape:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"âœ“ Model explains {test_r2*100:.2f}% of price variance\")\n",
    "print(f\"âœ“ Average prediction error: â‚¹{test_mae:.2f} Lakhs\")\n",
    "print(f\"âœ“ Predictions are off by ~{mape:.1f}% on average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8cd6f",
   "metadata": {},
   "source": [
    "## 12. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTOP 10 MOST IMPORTANT FEATURES:\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"{row['Feature']:40s} {row['Importance']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc5643",
   "metadata": {},
   "source": [
    "## 13. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "sample_predictions = pd.DataFrame({\n",
    "    'Actual Price (Lakhs)': y_test.values[:10],\n",
    "    'Predicted Price (Lakhs)': y_test_pred[:10],\n",
    "    'Difference': y_test.values[:10] - y_test_pred[:10],\n",
    "    'Error %': np.abs((y_test.values[:10] - y_test_pred[:10]) / y_test.values[:10] * 100)\n",
    "})\n",
    "\n",
    "print(\"\\nSAMPLE PREDICTIONS (First 10 Test Examples):\")\n",
    "print(\"=\"*80)\n",
    "print(sample_predictions.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3af609",
   "metadata": {},
   "source": [
    "## 14. Save Model with Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3847f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, scaler, and feature names using joblib\n",
    "joblib.dump(model, 'house_price_model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(X_encoded.columns.tolist(), 'feature_names.joblib')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… MODEL SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: Random Forest Regressor\")\n",
    "print(f\"Test RÂ² Score: {test_r2:.4f}\")\n",
    "print(f\"Test RMSE: â‚¹{test_rmse:.2f} Lakhs\")\n",
    "print(f\"Test MAE: â‚¹{test_mae:.2f} Lakhs\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(\"  1. house_price_model.joblib\")\n",
    "print(\"  2. scaler.joblib\")\n",
    "print(\"  3. feature_names.joblib\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792eee6",
   "metadata": {},
   "source": [
    "## 15. Load and Test Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9396a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = joblib.load('house_price_model.joblib')\n",
    "loaded_scaler = joblib.load('scaler.joblib')\n",
    "loaded_features = joblib.load('feature_names.joblib')\n",
    "\n",
    "# Test with sample data\n",
    "sample_data = X_test_scaled[:5]\n",
    "predictions = loaded_model.predict(sample_data)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”® TESTING LOADED MODEL\")\n",
    "print(\"=\"*60)\n",
    "for i, pred in enumerate(predictions):\n",
    "    actual = y_test.values[i]\n",
    "    error_pct = (abs(actual-pred)/actual*100) if actual != 0 else 0\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Actual:    â‚¹{actual:.2f} Lakhs\")\n",
    "    print(f\"  Predicted: â‚¹{pred:.2f} Lakhs\")\n",
    "    print(f\"  Error:     {error_pct:.2f}%\")\n",
    "    print()\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… Model loaded and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f136f2",
   "metadata": {},
   "source": [
    "## 16. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f325815",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“ Dataset:\")\n",
    "print(f\"  â€¢ Total samples: {len(df):,}\")\n",
    "print(f\"  â€¢ Features used: {X_encoded.shape[1]}\")\n",
    "print(f\"  â€¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"  â€¢ Test samples: {len(X_test):,}\")\n",
    "print(f\"\\nðŸŽ¯ Model Performance:\")\n",
    "print(f\"  â€¢ Algorithm: Random Forest Regressor\")\n",
    "print(f\"  â€¢ RÂ² Score: {test_r2:.4f} ({test_r2*100:.2f}% variance explained)\")\n",
    "print(f\"  â€¢ RMSE: â‚¹{test_rmse:.2f} Lakhs\")\n",
    "print(f\"  â€¢ MAE: â‚¹{test_mae:.2f} Lakhs\")\n",
    "print(f\"  â€¢ MAPE: {mape:.2f}%\")\n",
    "print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "print(f\"  â€¢ Average prediction error: â‚¹{test_mae:.2f} Lakhs\")\n",
    "print(f\"  â€¢ Model is production-ready\")\n",
    "print(f\"  â€¢ Cardinality reduced for memory efficiency\")\n",
    "print(f\"  â€¢ Saved with joblib for faster loading\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
